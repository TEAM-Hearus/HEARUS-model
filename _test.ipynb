{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPnglcUSNYc4g4guU/8MNZz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YO5Cs0kl0XqD"},"outputs":[],"source":["# 필요한 라이브러리 설치\n","!pip install transformers\n","!pip install torch\n","!pip install transformers torch\n","!pip install konlpy scikit-learn\n","!pip install kobert-transformers\n","!pip install bert-extractive-summarizer\n","!pip install transformers --upgrade\n","!pip install scikit-learn\n","\n","# 구글 드라이브 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from google.colab import drive\n","from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n","from konlpy.tag import Okt\n","import pandas as pd\n","import re\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification\n","\n"],"metadata":{"id":"UMdaoFig1kb_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu122\n"],"metadata":{"id":"Ilfvw3W6NB2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install accelerate -U\n","\n","# 또는 transformers[torch]를 설치하는 경우 (이 명령은 필요한 모든 의존성을 설치합니다)\n","!pip install transformers[torch]"],"metadata":{"id":"1CO-evVCOEcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""],"metadata":{"id":"l_YuJX9uQh5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 로드 및 전처리를 위한 함수 정의\n","def load_data(file_paths):\n","    dataframes = []\n","    for file_path in file_paths:\n","        df = pd.read_csv(file_path)\n","        dataframes.append(df)\n","    return pd.concat(dataframes, ignore_index=True)\n","\n","def preprocess_text(text, stopwords):\n","    text = re.sub(r'[^가-힣\\s]', '', text)  # 특수문자 제거, 한글만 남김\n","    okt = Okt()\n","    tokens = okt.morphs(text, stem=True)  # 형태소 분석\n","    tokens = [token for token in tokens if token not in stopwords]\n","    return ' '.join(tokens)\n","\n","# 데이터셋 클래스 정의\n","class T5FineTuningDataset(Dataset):\n","    def __init__(self, tokenizer, data, source_max_token_len=512, target_max_token_len=128):\n","        self.tokenizer = tokenizer\n","        self.data = data\n","        self.source_max_token_len = source_max_token_len\n","        self.target_max_token_len = target_max_token_len\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        data_row = self.data.iloc[index]\n","\n","        source_encoding = tokenizer(\n","            preprocess_text(data_row['content'], stopwords),\n","            max_length=self.source_max_token_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            add_special_tokens=True,\n","            return_tensors='pt'\n","        )\n","\n","        target_encoding = tokenizer(\n","            data_row['label'],\n","            max_length=self.target_max_token_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            add_special_tokens=True,\n","            return_tensors='pt'\n","        )\n","\n","        labels = target_encoding['input_ids']\n","        labels[labels == 0] = -100  # T5에서 padding token은 무시하기 위해 -100으로 설정\n","\n","        return dict(\n","            input_ids=source_encoding['input_ids'].flatten(),\n","            attention_mask=source_encoding['attention_mask'].flatten(),\n","            labels=labels.flatten()\n","        )\n","\n","# 파일 경로 설정\n","file_paths = [\n","    '/content/drive/MyDrive/fine/combined_text_with_labels.csv',\n","    '/content/drive/MyDrive/fine/combined_text_with_labels2.csv',\n","    '/content/drive/MyDrive/fine/combined_text_with_labels3.csv',\n","    '/content/drive/MyDrive/fine/combined_text_with_labels4.csv'\n","    # 다른 파일 경로를 추가하세요.\n","]\n","\n","# 데이터 로드\n","data = load_data(file_paths)\n","\n","# 불용어 설정\n","stopwords = set([\"의\", \"가\", \"이\", \"은\", \"들\", \"는\", \"좀\", \"잘\", \"걍\", \"과\", \"도\", \"를\", \"으로\", \"자\", \"에\", \"와\", \"한\", \"하다\"])\n","\n","# 토크나이저 및 모델 로드\n","tokenizer = T5Tokenizer.from_pretrained('digit82/kolang-t5-base')\n","model = T5ForConditionalGeneration.from_pretrained('digit82/kolang-t5-base')\n","\n","# 데이터셋 준비\n","dataset = T5FineTuningDataset(tokenizer, data)\n","\n","# 훈련 설정\n","training_args = TrainingArguments(\n","    output_dir='/content/drive/MyDrive/Model/trained_model',\n","    num_train_epochs=3,  # 적절한 에포크로 조정하세요.\n","    per_device_train_batch_size=8,  # 하드웨어에 맞게 조정하세요.\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='/content/drive/MyDrive/Model/logs',\n","    logging_steps=10,\n",")\n","\n","# 트레이너 초기화 및 훈련 시작\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset,\n",")\n","\n","trainer.train()\n","\n","\n"],"metadata":{"id":"F4bEGaU-2ABa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","print(torch.__version__)\n"],"metadata":{"id":"zpOzSP0BPH0m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(torch.version.cuda)\n"],"metadata":{"id":"3rJXedt-RLWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc --version"],"metadata":{"id":"bfCPrHZmRY0P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n"],"metadata":{"id":"5v0f8s4gR64R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 로드 및 전처리를 위한 함수 정의\n","def load_data(file_paths):\n","    dataframes = []\n","    for file_path in file_paths:\n","        df = pd.read_csv(file_path)\n","        dataframes.append(df)\n","    return pd.concat(dataframes, ignore_index=True)\n","\n","def preprocess_text(text, stopwords):\n","    text = re.sub(r'[^가-힣\\s]', '', text)  # 특수문자 제거, 한글만 남김\n","    okt = Okt()\n","    tokens = okt.morphs(text, stem=True)  # 형태소 분석\n","    tokens = [token for token in tokens if token not in stopwords]\n","    return ' '.join(tokens)\n","\n","# 데이터셋 클래스 정의\n","class KoBertDataset(Dataset):\n","    def __init__(self, tokenizer, data, max_token_len=256):\n","        self.tokenizer = tokenizer\n","        self.data = data\n","        self.max_token_len = max_token_len\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        data_row = self.data.iloc[index]\n","\n","        encoding = tokenizer(\n","            preprocess_text(data_row['content'], stopwords),\n","            add_special_tokens=True,\n","            max_length=self.max_token_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return dict(\n","            input_ids=encoding['input_ids'].flatten(),\n","            attention_mask=encoding['attention_mask'].flatten(),\n","            labels=torch.tensor(data_row['label'])\n","        )\n","\n","# 파일 경로 설정\n","file_paths = [\n","    '/content/drive/MyDrive/fine/combined_text_with_labels.csv',\n","    '/content/drive/MyDrive/fine/combined_text_with_labels2.csv',\n","    '/content/drive/MyDrive/fine/combined_text_with_labels3.csv',\n","    '/content/drive/MyDrive/fine/combined_text_with_labels4.csv'\n","\n","]\n","\n","# 데이터 로드\n","data = load_data(file_paths)\n","\n","# 불용어 설정\n","stopwords = set([\"의\", \"가\", \"이\", \"은\", \"들\", \"는\", \"좀\", \"잘\", \"걍\", \"과\", \"도\", \"를\", \"으로\", \"자\", \"에\", \"와\", \"한\", \"하다\"])\n","\n","# 토크나이저 및 모델 로드\n","tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n","model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2)  # num_labels는 데이터셋의 레이블 수에 맞춰 조정\n","\n","# 데이터셋 준비\n","dataset = KoBertDataset(tokenizer, data)\n","\n","# 훈련 설정\n","training_args = TrainingArguments(\n","    output_dir='/content/drive/MyDrive/Model/trained_model',\n","    num_train_epochs=3,  # 적절한 에포크로 조정하세요.\n","    per_device_train_batch_size=8,  # 하드웨어에 맞게 조정하세요.\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='/content/drive/MyDrive/Model/logs',\n","    logging_steps=10,\n",")\n","\n","# 트레이너 초기화 및 훈련 시작\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset,\n",")\n","\n","trainer.train()"],"metadata":{"id":"Rjilb8vO3Ckv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""],"metadata":{"id":"B9isQxyd4J8v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"acULRFoE45LR"},"execution_count":null,"outputs":[]}]}