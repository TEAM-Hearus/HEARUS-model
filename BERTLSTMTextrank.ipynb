{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyM71n+No2QFOd5osgn8ksKr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ko7dVJwTMA3O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install konlpy networkx"],"metadata":{"id":"B_DMT-zZMWtp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from transformers import BertModel, BertTokenizer\n","import json\n","import nltk\n","import pandas as pd\n","from konlpy.tag import Okt\n","import networkx as nx\n","nltk.download('punkt')\n","\n","# BertSumLSTM 모델 정의\n","class BertSumLSTM(nn.Module):\n","    def __init__(self, bert_model_name, hidden_dim, num_layers, dropout):\n","        super(BertSumLSTM, self).__init__()\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","        self.lstm = nn.LSTM(self.bert.config.hidden_size, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n","        self.classifier = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        sequence_output = outputs.last_hidden_state\n","        lstm_output, _ = self.lstm(sequence_output)\n","        logits = self.classifier(lstm_output).squeeze(-1)\n","        return logits\n","\n","# 중요한 문장 추출 함수\n","def extract_important_sentences(text, model, tokenizer, num_sentences=2):\n","    model.eval()\n","    with torch.no_grad():\n","        input_ids, attention_masks, sentences = create_input(text, tokenizer)\n","        logits = model(input_ids, attention_masks)\n","        scores = torch.sigmoid(logits).squeeze().tolist()\n","\n","        important_sentence_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]\n","        important_sentences = [sentences[i] for i in important_sentence_indices]\n","        return important_sentences\n","\n","# 입력 생성 함수\n","def create_input(text, tokenizer):\n","    sentences = nltk.sent_tokenize(text)\n","    input_ids = []\n","    attention_masks = []\n","\n","    for sent in sentences:\n","        encoded_dict = tokenizer.encode_plus(sent, add_special_tokens=True, max_length=35, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n","        input_ids.append(encoded_dict['input_ids'])\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","\n","    return input_ids, attention_masks, sentences\n","\n","# 명사 추출 함수\n","def extract_nouns(text, stopwords):\n","    okt = Okt()\n","    nouns = okt.nouns(text)\n","    filtered_nouns = [noun for noun in nouns if noun not in stopwords and len(noun) > 1]\n","    return filtered_nouns\n","\n","# 텍스트 랭크 알고리즘 적용 함수\n","def apply_text_rank(nouns):\n","    graph = nx.Graph()\n","    graph.add_nodes_from(nouns)\n","\n","    for i in range(len(nouns)):\n","        for j in range(i + 1, len(nouns)):\n","            graph.add_edge(nouns[i], nouns[j])\n","\n","    scores = nx.pagerank(graph)\n","    return scores\n","\n","# 모델 및 토크나이저 초기화\n","bert_model_name = 'bert-base-multilingual-cased'\n","hidden_dim = 256\n","num_layers = 2\n","dropout = 0.3\n","sentence_model = BertSumLSTM(bert_model_name, hidden_dim, num_layers, dropout)\n","tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n","\n","# 불용어 목록\n","stopwords = ['이', '그', '저', '것', '수', '등', '해', '있', '되', '없', '않', '같', '에서', '로', '고', '으로', '다', '만', '도', '의', '가', '이런', '저런', '합니다', '하세요']\n","\n","# JSON 파일 경로\n","file_path = '/content/drive/MyDrive/fine/computer.json'\n","\n","# JSON 파일을 데이터프레임으로 불러오기\n","df = pd.read_json(file_path)\n","\n","# 'sumText' 컬럼의 첫 번째 값을 사용하여 새로운 데이터프레임 생성\n","first_sumText = df['sumText'].iloc[0]\n","single_row_df = pd.DataFrame([first_sumText], columns=['sumText'])\n","\n","# 'sumText' 컬럼의 모든 항목을 결합하여 전체 텍스트 생성\n","full_text = ' '.join([' '.join(item) if isinstance(item, list) else item for item in df['sumText']])\n","\n","\n","# 결과를 저장합니다.\n","output_file_path = '/content/drive/MyDrive/fine/single_summary_output4.json'\n","single_row_df.to_json(output_file_path, force_ascii=False)\n","\n","# JSON 파일을 불러옵니다.\n","file_path = '/content/drive/MyDrive/fine/single_summary_output4.json'\n","df = pd.read_json(file_path)\n","\n","# 중요한 문장과 단어 추출\n","important_sentences = []\n","important_words_list = []\n","\n","for text in df['sumText']:\n","    imp_sentences = extract_important_sentences(text, sentence_model, tokenizer, num_sentences=2)\n","    important_sentences.append(imp_sentences)\n","\n","    nouns = extract_nouns(text, stopwords)\n","    scores = apply_text_rank(nouns)\n","    imp_words = sorted(scores, key=scores.get, reverse=True)[:5]\n","    important_words_list.append(imp_words)\n","\n","# 결과 저장\n","df['important_sentence'] = important_sentences\n","df['important_words'] = important_words_list\n","\n","# JSON 파일로 저장\n","output_file_path = '/content/drive/MyDrive/fine/summary1.json'\n","df.to_json(output_file_path, force_ascii=False)"],"metadata":{"id":"4EORw0GtN7Nn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from ast import literal_eval\n","\n","class Example:\n","    def __init__(self, df_original, important_sentence, important_words):\n","        self.df_original = df_original\n","        self.important_sentence = self.tokenize_and_eval(important_sentence)\n","        self.important_words = self.tokenize_and_eval(important_words)\n","\n","    def tokenize_and_eval(self, text):\n","        if isinstance(text, str):\n","            # 문자열을 공백을 기준으로 분할하고 리스트로 반환\n","            return text.split()\n","        else:\n","            # 이미 리스트 형태인 경우 그대로 반환\n","            return text\n","\n","    def update_unProcessedText(self):\n","        l = []\n","        for text_list in self.df_original['unProcessedText']:\n","            if text_list[0] in self.important_sentence[0]:\n","                l.append('highlight')\n","            elif text_list[0] in set(self.important_words):\n","                l.append('comment')\n","            else:\n","                l.append('none')\n","        for ind, unprocessedtext in enumerate(self.df_original['unProcessedText']):\n","            unprocessedtext[1] = l[ind]\n","\n","    def clear_other_rows(self):\n","        for col in self.df_original.columns:\n","            if col != 'unProcessedText':\n","                self.df_original.loc[1:, col] = np.nan\n","\n","    def save_to_json(self, path):\n","        self.df_original.to_json(path, force_ascii=False)\n","\n","# 중요 문장과 단어 추출\n","file_path = '/content/drive/MyDrive/fine/summary1.json'\n","df = pd.read_json(file_path)\n","important_sentence = df.loc[0, 'important_sentence']\n","important_words = df.loc[0, 'important_words']\n","\n","# 원본 DataFrame 로드\n","original_file_path = '/content/drive/MyDrive/fine/computer.json'\n","df_original = pd.read_json(original_file_path)\n","\n","# Example 객체 생성 및 데이터 처리\n","exam1 = Example(df_original, important_sentence, important_words)\n","exam1.update_unProcessedText()\n","exam1.clear_other_rows()\n","\n","# 수정된 DataFrame을 새로운 JSON 파일로 저장\n","output_file_path = '/content/drive/MyDrive/fine/updated100.json'\n","exam1.save_to_json(output_file_path)"],"metadata":{"id":"VsdFWnpUg6eg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"urjFfUnjkmaP"},"execution_count":null,"outputs":[]}]}